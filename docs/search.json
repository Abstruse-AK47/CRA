[
  {
    "objectID": "main.html",
    "href": "main.html",
    "title": "Credit Risk Assesment using bayesian Decision Thoery with noisy Financial Data",
    "section": "",
    "text": "Code\n#DataFrame Libraries\nimport pandas as pd\nimport numpy as np\n\n#Plot Libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno"
  },
  {
    "objectID": "main.html#load-uci-credit-approval-dataset",
    "href": "main.html#load-uci-credit-approval-dataset",
    "title": "Credit Risk Assesment using bayesian Decision Thoery with noisy Financial Data",
    "section": "Load UCI Credit Approval dataset",
    "text": "Load UCI Credit Approval dataset\n\n\nCode\nurl = \"https://archive.ics.uci.edu/ml/machine-learning-databases/credit-screening/crx.data\"\ncolumns = [\"Gender\", \"Age\", \"Debt\", \"Married\", \"BankCustomer\", \n           \"EducationLevel\", \"Ethnicity\", \"YearsEmployed\", \n           \"PriorDefault\", \"Employed\", \"CreditScore\", \n           \"DriversLicense\", \"Citizen\", \"ZipCode\", \"Income\", \"Approved\"]\n\n# Replace '?' with NaN\ndata = pd.read_csv(url, names=columns, na_values=\"?\")\n\ndata.head()\n\n\n\n\n\n\n\n\n\nGender\nAge\nDebt\nMarried\nBankCustomer\nEducationLevel\nEthnicity\nYearsEmployed\nPriorDefault\nEmployed\nCreditScore\nDriversLicense\nCitizen\nZipCode\nIncome\nApproved\n\n\n\n\n0\nb\n30.83\n0.000\nu\ng\nw\nv\n1.25\nt\nt\n1\nf\ng\n202.0\n0\n+\n\n\n1\na\n58.67\n4.460\nu\ng\nq\nh\n3.04\nt\nt\n6\nf\ng\n43.0\n560\n+\n\n\n2\na\n24.50\n0.500\nu\ng\nq\nh\n1.50\nt\nf\n0\nf\ng\n280.0\n824\n+\n\n\n3\nb\n27.83\n1.540\nu\ng\nw\nv\n3.75\nt\nt\n5\nt\ng\n100.0\n3\n+\n\n\n4\nb\n20.17\n5.625\nu\ng\nw\nv\n1.71\nt\nf\n0\nf\ns\n120.0\n0\n+"
  },
  {
    "objectID": "main.html#add-noise-to-income-column",
    "href": "main.html#add-noise-to-income-column",
    "title": "Credit Risk Assesment using bayesian Decision Thoery with noisy Financial Data",
    "section": "Add noise to ‘Income’ column",
    "text": "Add noise to ‘Income’ column\n\nSimulating Noisy Features:\n→ Added Gaussian noise to numeric columns(eg.,income):\n\n\n\nCode\nnoise_level = 0.1  # 10% noise\ndata[\"Income_noisy\"] = data[\"Income\"] + np.random.normal(0, noise_level * data[\"Income\"].std(), len(data))\n\n# Replace 'Income' with noisy version for simulation\ndata[\"Income\"] = data[\"Income_noisy\"]\ndata.drop(columns=[\"Income_noisy\"], inplace=True)"
  },
  {
    "objectID": "main.html#finding-relevant-and-handling-missing-data",
    "href": "main.html#finding-relevant-and-handling-missing-data",
    "title": "Credit Risk Assesment using bayesian Decision Thoery with noisy Financial Data",
    "section": "Finding Relevant and Handling Missing Data",
    "text": "Finding Relevant and Handling Missing Data\n\nSince the data we have does not include information on categorical(non numeric) variables, we have to rely on numerical data for the scope of this project.\nIdentify numerical columns\n\n\n\nCode\nnumerical_columns = data.select_dtypes(include=[\"number\"]).columns\nprint(numerical_columns)\n\n\nIndex(['Age', 'Debt', 'YearsEmployed', 'CreditScore', 'ZipCode', 'Income'], dtype='object')\n\n\n\nVisualize Missing Values\n\n\n\nCode\n# Visualize missing data\nmsno.matrix(data)\nplt.gcf().set_size_inches(7, 5)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 1: Missing Values Matrix\n\n\n\n\n\n\nSeparate numeric and non-numeric columns and calculate missing data\n\n\n\nCode\nfrom sklearn.impute import SimpleImputer\n\nnumeric_data = data.select_dtypes(include=[\"number\"])\ncategorical_data = data.select_dtypes(exclude=[\"number\"])\n\n# Impute numeric columns\nimputer_numeric = SimpleImputer(strategy=\"mean\")\nnumeric_data_imputed = pd.DataFrame(imputer_numeric.fit_transform(numeric_data), columns=numeric_data.columns)\n\n# Impute categorical columns (e.g., with the most frequent value)\nimputer_categorical = SimpleImputer(strategy=\"most_frequent\")\ncategorical_data_imputed = pd.DataFrame(imputer_categorical.fit_transform(categorical_data), columns=categorical_data.columns)\n\n# Combine numeric and categorical data\ndata_imputed = pd.concat([numeric_data_imputed, categorical_data_imputed], axis=1)\n\n# Check the final imputed dataset\ndata_imputed.head()\n\n\n\n\n\n\n\n\n\nAge\nDebt\nYearsEmployed\nCreditScore\nZipCode\nIncome\nGender\nMarried\nBankCustomer\nEducationLevel\nEthnicity\nPriorDefault\nEmployed\nDriversLicense\nCitizen\nApproved\n\n\n\n\n0\n30.83\n0.000\n1.25\n1.0\n202.0\n495.503183\nb\nu\ng\nw\nv\nt\nt\nf\ng\n+\n\n\n1\n58.67\n4.460\n3.04\n6.0\n43.0\n1652.228386\na\nu\ng\nq\nh\nt\nt\nf\ng\n+\n\n\n2\n24.50\n0.500\n1.50\n0.0\n280.0\n670.104196\na\nu\ng\nq\nh\nt\nf\nf\ng\n+\n\n\n3\n27.83\n1.540\n3.75\n5.0\n100.0\n-300.646435\nb\nu\ng\nw\nv\nt\nt\nt\ng\n+\n\n\n4\n20.17\n5.625\n1.71\n0.0\n120.0\n-895.352913\nb\nu\ng\nw\nv\nt\nf\nf\ns\n+\n\n\n\n\n\n\n\n\nDistribution of target variable\n\n\n\nCode\n# Plot the distribution of the target variable\nsns.countplot(data=data_imputed, x=\"Approved\",hue = \"Approved\", palette=\"viridis\")\nplt.xlabel(\"Approval Status\")\nplt.ylabel(\"Count\")\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2: Distribution of Target Variable (Approved)"
  },
  {
    "objectID": "main.html#bayesian-classifier-implementation",
    "href": "main.html#bayesian-classifier-implementation",
    "title": "Credit Risk Assesment using bayesian Decision Thoery with noisy Financial Data",
    "section": "Bayesian Classifier Implementation",
    "text": "Bayesian Classifier Implementation\n\nDefine Priors:\n→ Calculate prior probabilities\n\n\n\nCode\nprior_approved = data[\"Approved\"].value_counts(normalize=True)[\"+\"]\nprior_denied = data[\"Approved\"].value_counts(normalize=True)[\"-\"]\n\nprint(f\"Prior P(Approved): {prior_approved}, P(Denied): {prior_denied}\")\n\n\nPrior P(Approved): 0.4449275362318841, P(Denied): 0.5550724637681159\n\n\n\nLikelihood Estimation:\n→ Fit a Gaussian for ‘Income’ by class\n\n\n\nCode\nfrom scipy.stats import norm\n\napproved_income = data[data[\"Approved\"] == \"+\"][\"Income\"]\ndenied_income = data[data[\"Approved\"] == \"-\"][\"Income\"]\n\nlikelihood_approved = norm(approved_income.mean(), approved_income.std())\nlikelihood_denied = norm(denied_income.mean(), denied_income.std())\n\n\n\nPosterior Calculation:\n→ Using Bayes’ theorum to compute posterior porbabilities:\n\\[P(\\text{Approved}|\\text{Income})=\\frac{P(\\text{Income}|\\text{Approved})\\cdot P(\\text{Approved})}{P(\\text{Income})}\\]\n\n\n\nCode\ndef posterior(income, prior, likelihood):\n    return prior * likelihood.pdf(income)\n\n# New applicant with income=5000\nnew_income = 5000\np_approved = posterior(new_income, prior_approved, likelihood_approved)\np_denied = posterior(new_income, prior_denied, likelihood_denied)\n\n# Normalize posteriors\ntotal_prob = p_approved + p_denied\np_approved /= total_prob\np_denied /= total_prob\n\nprint(f\"P(Approved|Income): {p_approved}, P(Denied|Income): {p_denied}\")\n\n\nP(Approved|Income): 0.9999993041685493, P(Denied|Income): 6.958314505503845e-07\n\n\n\nMap Decision:\n→ The class with highest posterior probability:\n\n\n\nCode\ndecision = \"Approved\" if p_approved &gt; p_denied else \"Denied\"\nprint(f\"Decision: {decision}\")\n\n\nDecision: Approved\n\n\n\nCost-Sensitive Decision:\n→ Incoporated a cost matrix(eg - false negatives are more costly):\n\n\n\nCode\n# Cost matrix: [ [TN, FP], [FN, TP] ]\ncost_matrix = [[0, 1], [5, 0]]  # High cost for FN\n\n# expected cost\nexpected_cost_approved = cost_matrix[1][0] * p_denied  # FN cost\nexpected_cost_denied = cost_matrix[0][1] * p_approved  # FP cost\n\nfinal_decision = \"Approved\" if expected_cost_approved &lt; expected_cost_denied else \"Denied\"\nprint(f\"Cost-sensitive Decision: {final_decision}\")\n\n\nCost-sensitive Decision: Approved"
  },
  {
    "objectID": "main.html#model-evaulation",
    "href": "main.html#model-evaulation",
    "title": "Credit Risk Assesment using bayesian Decision Thoery with noisy Financial Data",
    "section": "Model Evaulation",
    "text": "Model Evaulation\n\nEvaulated the models performance with metrics like accuracy, precision, and recall.\n\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\n# Split the data into features and target\nX = data_imputed.drop(columns=[\"Approved\"])\ny = data_imputed[\"Approved\"]\n\n# Identify categorical columns\ncategorical_columns = X.select_dtypes(include=[\"object\"]).columns\n\n# Apply one-hot encoding to categorical columns\nX_encoded = pd.get_dummies(X, columns=categorical_columns, drop_first=True)\n\n\n\n# Split the dataset into training and test sets (70% train, 30% test)\nX_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.3, random_state=42)\n\n# Initialize the Naive Bayes classifier\nmodel = GaussianNB()\n\n# Train the model\nmodel.fit(X_train, y_train)\n\n# Make predictions on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred, pos_label='+')\nrecall = recall_score(y_test, y_pred, pos_label='+')"
  },
  {
    "objectID": "main.html#results",
    "href": "main.html#results",
    "title": "Credit Risk Assesment using bayesian Decision Thoery with noisy Financial Data",
    "section": "Results",
    "text": "Results\n\n\nCode\nfrom sklearn.metrics import f1_score\n\nf1 = f1_score(y_test, y_pred, pos_label='+')\nprint(f\"F1 Score: {f1:.4f}\")\nprint(f\"Accuracy: {accuracy:.4f}\")\nprint(f\"Precision: {precision:.4f}\")\nprint(f\"Recall: {recall:.4f}\")\n\n\nF1 Score: 0.7802\nAccuracy: 0.8068\nPrecision: 0.8353\nRecall: 0.7320"
  }
]